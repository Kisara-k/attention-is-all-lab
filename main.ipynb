{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:54:16.380015Z","iopub.execute_input":"2025-07-17T16:54:16.380167Z","iopub.status.idle":"2025-07-17T16:54:20.773413Z","shell.execute_reply.started":"2025-07-17T16:54:16.380153Z","shell.execute_reply":"2025-07-17T16:54:20.772548Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:54:50.776580Z","iopub.execute_input":"2025-07-17T16:54:50.777031Z","iopub.status.idle":"2025-07-17T16:54:50.783258Z","shell.execute_reply.started":"2025-07-17T16:54:50.777005Z","shell.execute_reply":"2025-07-17T16:54:50.782463Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask=None):\n    d_k = q.size(-1)\n    scores = q @ k.transpose(-2, -1) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    attn = torch.softmax(scores, dim=-1)\n    return attn @ v, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:55:01.626855Z","iopub.execute_input":"2025-07-17T16:55:01.627667Z","iopub.status.idle":"2025-07-17T16:55:01.633320Z","shell.execute_reply.started":"2025-07-17T16:55:01.627633Z","shell.execute_reply":"2025-07-17T16:55:01.632510Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.out = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        B, T, _ = q.size()\n\n        def transform(x, linear):\n            B, T, _ = x.size()\n            x = linear(x)\n            return x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n\n        q = transform(q, self.q_linear)\n        k = transform(k, self.k_linear)\n        v = transform(v, self.v_linear)\n\n        scores, attn = scaled_dot_product_attention(q, k, v, mask)\n        scores = scores.transpose(1, 2).contiguous().view(B, T, -1)\n        return self.out(scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:17.658050Z","iopub.execute_input":"2025-07-17T17:10:17.658345Z","iopub.status.idle":"2025-07-17T17:10:17.667644Z","shell.execute_reply.started":"2025-07-17T17:10:17.658324Z","shell.execute_reply":"2025-07-17T17:10:17.666746Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:17.977529Z","iopub.execute_input":"2025-07-17T17:10:17.978239Z","iopub.status.idle":"2025-07-17T17:10:17.982976Z","shell.execute_reply.started":"2025-07-17T17:10:17.978213Z","shell.execute_reply":"2025-07-17T17:10:17.982172Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        x = self.norm1(x + self.attn(x, x, x, mask))\n        x = self.norm2(x + self.ff(x))\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:18.309174Z","iopub.execute_input":"2025-07-17T17:10:18.309933Z","iopub.status.idle":"2025-07-17T17:10:18.316559Z","shell.execute_reply.started":"2025-07-17T17:10:18.309906Z","shell.execute_reply":"2025-07-17T17:10:18.315788Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.enc_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        x = self.norm1(x + self.self_attn(x, x, x, tgt_mask))\n        x = self.norm2(x + self.enc_attn(x, enc_out, enc_out, src_mask))\n        x = self.norm3(x + self.ff(x))\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n\n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:18.636591Z","iopub.execute_input":"2025-07-17T17:10:18.636901Z","iopub.status.idle":"2025-07-17T17:10:18.644344Z","shell.execute_reply.started":"2025-07-17T17:10:18.636879Z","shell.execute_reply":"2025-07-17T17:10:18.643546Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.encoder = Encoder(src_vocab, d_model, num_layers, num_heads, d_ff, dropout)\n        self.decoder = Decoder(tgt_vocab, d_model, num_layers, num_heads, d_ff, dropout)\n        self.fc_out = nn.Linear(d_model, tgt_vocab)\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        enc_out = self.encoder(src, src_mask)\n        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n        return self.fc_out(dec_out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:19.635644Z","iopub.execute_input":"2025-07-17T17:10:19.636539Z","iopub.status.idle":"2025-07-17T17:10:19.642843Z","shell.execute_reply.started":"2025-07-17T17:10:19.636505Z","shell.execute_reply":"2025-07-17T17:10:19.641961Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def create_padding_mask(seq, pad_token=0):\n    # seq: (batch_size, seq_len)\n    return (seq != pad_token).unsqueeze(1).unsqueeze(2)  # shape: (B, 1, 1, T)\n\ndef create_look_ahead_mask(size):\n    return torch.triu(torch.ones((size, size)), diagonal=1).bool()\n\ndef create_combined_mask(tgt_seq, pad_token=0):\n    padding_mask = create_padding_mask(tgt_seq, pad_token)  # shape: (B, 1, 1, T)\n    look_ahead_mask = create_look_ahead_mask(tgt_seq.size(1)).to(tgt_seq.device)  # shape: (T, T)\n    look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(1)  # shape: (1, 1, T, T)\n    return padding_mask & ~look_ahead_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:10:19.919033Z","iopub.execute_input":"2025-07-17T17:10:19.919579Z","iopub.status.idle":"2025-07-17T17:10:19.924539Z","shell.execute_reply.started":"2025-07-17T17:10:19.919555Z","shell.execute_reply":"2025-07-17T17:10:19.923767Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"PAD_TOKEN_ID = 0\nVOCAB_SIZE = 5000\nSEQ_LEN = 32         # Max length of input/output token sequences\nBATCH_SIZE = 16\nNUM_EPOCHS = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = Transformer(\n    src_vocab=5000,\n    tgt_vocab=5000,\n    d_model=512,\n    num_layers=6,\n    num_heads=8,\n    d_ff=2048\n).to(device)\n\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:11:35.437395Z","iopub.execute_input":"2025-07-17T17:11:35.438179Z","iopub.status.idle":"2025-07-17T17:11:36.068948Z","shell.execute_reply.started":"2025-07-17T17:11:35.438153Z","shell.execute_reply":"2025-07-17T17:11:36.068071Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass DummyTranslationDataset(Dataset):\n    def __init__(self, num_samples=1000, seq_len=SEQ_LEN, vocab_size=VOCAB_SIZE):\n        self.num_samples = num_samples\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        src = torch.randint(1, self.vocab_size, (self.seq_len,))\n        tgt = torch.randint(1, self.vocab_size, (self.seq_len,))\n        src[torch.randint(0, self.seq_len, (1,))] = PAD_TOKEN_ID  # randomly add a PAD\n        tgt[torch.randint(0, self.seq_len, (1,))] = PAD_TOKEN_ID\n        return {'src': src, 'tgt': tgt}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:11:52.558043Z","iopub.execute_input":"2025-07-17T17:11:52.558335Z","iopub.status.idle":"2025-07-17T17:11:52.564263Z","shell.execute_reply.started":"2025-07-17T17:11:52.558312Z","shell.execute_reply":"2025-07-17T17:11:52.563470Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"dataset = DummyTranslationDataset()\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:11:55.649288Z","iopub.execute_input":"2025-07-17T17:11:55.649550Z","iopub.status.idle":"2025-07-17T17:11:55.653648Z","shell.execute_reply.started":"2025-07-17T17:11:55.649532Z","shell.execute_reply":"2025-07-17T17:11:55.652864Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"for epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch in dataloader:\n        src = batch['src'].to(device)\n        tgt = batch['tgt'].to(device)\n\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        src_mask = create_padding_mask(src)\n        tgt_mask = create_combined_mask(tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask)\n        logits = logits.view(-1, logits.size(-1))\n        tgt_output = tgt_output.contiguous().view(-1)\n\n        loss = loss_fn(logits, tgt_output)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T17:11:56.728532Z","iopub.execute_input":"2025-07-17T17:11:56.729120Z","iopub.status.idle":"2025-07-17T17:12:12.602782Z","shell.execute_reply.started":"2025-07-17T17:11:56.729095Z","shell.execute_reply":"2025-07-17T17:12:12.602075Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Loss: nan\nEpoch 2 | Loss: nan\nEpoch 3 | Loss: nan\nEpoch 4 | Loss: nan\nEpoch 5 | Loss: nan\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}